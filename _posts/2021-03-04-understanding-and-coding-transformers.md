---
title: "(Under Construction) Understanding and Coding Transformers"
categories:
  - Posts
tags:
  - TensorFlow
  - Machine Learning
collections:
  - Reflection
excerpt: "Examining the associated paper as well as various codebases."
toc: true
toc_sticky: true
---
> Note: The purpose of this post is as a personal reflection and not as a tutorial.

# Some Useful Links

## The Paper "Attention Is All You Need"

[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)

## TensorFlow Tutorial

* Transformer model for language understanding

[https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer)

## Tensor2Tensor

[https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)

This was mentioned in the transformer paper as the original codebase.

"Tensor2Tensor, or T2T for short, is a library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."

It has since been replaced by Trax.

* Automated Speech Recognition with the Transformer model

This was the only tutorial under the docs in github

[https://cloud.google.com/tpu/docs/tutorials/automated-speech-recognition](https://cloud.google.com/tpu/docs/tutorials/automated-speech-recognition)

## Trax

[https://github.com/google/trax](https://github.com/google/trax)

"Trax is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the Google Brain team".

Trax Documentation

[https://trax-ml.readthedocs.io/en/latest/](https://trax-ml.readthedocs.io/en/latest/)
